{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df53ac97",
   "metadata": {},
   "source": [
    "# A computational deep learning investigation of animacy perception in the human brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cfebe4",
   "metadata": {},
   "source": [
    "# 1. FINETUNE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef280759",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "### Fine-tuning a Pretrained AlexNet on a Custom Two-Class Dataset\n",
    "\n",
    "This script demonstrates how to fine-tune a pretrained AlexNet neural network on a custom dataset\n",
    "with only two categories. All layers except FC7 and the output layer are frozen. The learning rate\n",
    "starts at 0.01 and gradually multiplies with a factor of 0.1 if the validation loss does not improve\n",
    "over 5 consecutive epochs, with a minimum learning rate of 0.00001. The batch size is set to 20, and\n",
    "the training is run for 500 epochs. We use the SGD optimization algorithm, cross-entropy loss\n",
    "function, momentum of 0.9, and shuffle the data each epoch.\n",
    "\n",
    "### Obtaining a correlation matrix\n",
    "\n",
    "The second part of the script uses the trained model and a novel test set to obtain image vectors from the output layer. The image vectors are correlated with each other and the resulting dissimilarity matrix is displayed. \n",
    "\n",
    "### Check this before running the code :)\n",
    "Download, extract and save the dataset: https://osf.io/t6apv\n",
    "\n",
    "Dowload and save the pth weights file for the model finetuned in this script: https://osf.io/yk7jf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de3e61d",
   "metadata": {},
   "source": [
    "## Set dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d10672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your custom dataset directory\n",
    "#data_dir = '/path/to/your/dataset/'\n",
    "data_dir = '/Users/duyckstefanie/Library/CloudStorage/OneDrive-KULeuven/Documents/Study4_forsharing/DATASETS/animal_bias_dataset'\n",
    "#pth_dir = 'path_to_finetuned_model.pth'\n",
    "pth_dir = '/Users/duyckstefanie/Library/CloudStorage/OneDrive-KULeuven/Documents/Study4_forsharing/OSF_modeltesting/weights/animalbias/alexnet/best_model_weights.pth'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96164d40",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c57bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188397e",
   "metadata": {},
   "source": [
    "## Define the data transforms for training, validation and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b1e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(5), \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(224), #imagenet standards\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([ #validation does not use augmentation\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d4a1c",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f770a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for training, validation and testing using ImageFolder\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    os.path.join(data_dir, 'train'),\n",
    "    data_transforms['train']\n",
    ")\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    os.path.join(data_dir, 'val'),\n",
    "    data_transforms['val']\n",
    ")\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    os.path.join(data_dir, 'test'),\n",
    "    data_transforms['test']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc108283",
   "metadata": {},
   "source": [
    "## Set model specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0ee58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "num_epochs = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125ebd4",
   "metadata": {},
   "source": [
    "## Create data loaders for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6976fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "    'val': torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3ef11",
   "metadata": {},
   "source": [
    "## Load the pre-trained AlexNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb824e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81217dbc",
   "metadata": {},
   "source": [
    "## Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfd1410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except FC7 and the output layer\n",
    "for name, param in model.named_parameters():\n",
    "    if name not in ['classifier.6.weight', 'classifier.6.bias']:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Modify the last fully connected layer for 2 classes\n",
    "model.classifier[6] = nn.Linear(4096, 2)\n",
    "\n",
    "# Set up the optimizer and learning rate scheduler\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, min_lr=0.00001)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357fbd06",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        corrects = 0\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if phase == 'val':\n",
    "            scheduler.step(epoch_loss)\n",
    "\n",
    "            if epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print()\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(best_model_wts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f026ed01",
   "metadata": {},
   "source": [
    "# 2. EVALUATE MODEL WITH TEST SET AND OBTAIN CORRELATION MATRIX FROM OUTPUT LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280f46c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac494a",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this piece if you wish to continue with model trained in previous cell\n",
    "#this part will load the pth weights file from the authors:\n",
    "model = models.alexnet(pretrained=True)\n",
    "model.classifier[6] = nn.Linear(4096, 2)  # Adjust this line based on your model architecture\n",
    "checkpoint = torch.load(pth_dir)\n",
    "model.load_state_dict(checkpoint['model_state_dict']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6d477",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9806ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dirs = list()\n",
    "temp_list = list()\n",
    "    \n",
    "for path, subdirs, files in os.walk(os.path.join(data_dir, 'test')):\n",
    "    #First path is just val folder, so exclude this\n",
    "    if subdirs == []:\n",
    "        for name in sorted(files):\n",
    "            # save image paths per species in list\n",
    "            temp_list.append(os.path.join(path, name))\n",
    "        image_dirs.append(temp_list)\n",
    "        temp_list = list() #empty temp_list\n",
    "\n",
    "image_dirs.reverse()        \n",
    "#image_dirs must be in the order of 1animate, 2lookalike, 3inanimate\n",
    "print(image_dirs)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ecad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_of_extraction = model.classifier[6]\n",
    "num_nodes_fc8 = len(checkpoint['model_state_dict']['classifier.6.weight'])\n",
    "print (num_nodes_fc8) #get number of output nodes in FC8\n",
    "\n",
    "# Transform & make tensor\n",
    "scaler = transforms.Resize(224)\n",
    "to_tensor = transforms.ToTensor()\n",
    "#The last transform ‘to_tensor’ will be used to convert the PIL image to a PyTorch tensor (multidimensional array).\n",
    "normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb003ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(image_name, nodesfc8):\n",
    "        # 1. Load the image with Pillow library\n",
    "        img = Image.open(image_name).convert('RGB')\n",
    "        print(image_name)\n",
    "        \n",
    "        # 2. Create a PyTorch Variable with the transformed image\n",
    "        t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0)) \n",
    "        \n",
    "        # 3. Create a vector of zeros that will hold our feature vector\n",
    "        my_embedding = torch.zeros(1, nodesfc8)   \n",
    "\n",
    "        # 4. Define a function that will copy the output of a layer\n",
    "        def copy_data(m, i, o):\n",
    "            my_embedding.copy_(o.data) \n",
    "    \n",
    "        # 5. Attach that function to our selected layer\n",
    "        h = layer_of_extraction.register_forward_hook(copy_data) \n",
    "        \n",
    "        # 6. Run the model on our transformed image\n",
    "        model(t_img) \n",
    "        \n",
    "        # 7. Detach our copy function from the layer\n",
    "        h.remove()   \n",
    "        \n",
    "        # 8. Return the feature vector\n",
    "        return my_embedding #.data.numpy().astype(DTYPE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e13f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vectors = [] \n",
    "save_imagenames = []\n",
    "matrix = np.zeros([99,99]) #create empty matrix to store results\n",
    "for species in range(3): # row index species,   \n",
    "    for image in range(33): #row and \n",
    "        image_path = image_dirs[species][image]\n",
    "        vector = get_vector(image_path, num_nodes_fc8) #Get vector of first image\n",
    "        vector = torch.flatten(vector)\n",
    "        \n",
    "        save_imagenames.append(image_path)\n",
    "        save_vectors.append(np.array(vector))\n",
    "     \n",
    "test3 = np.array(save_vectors).T\n",
    "vector_df = pd.DataFrame(test3) #save the exact output vector for each image\n",
    "imagenames_df = pd.DataFrame(save_imagenames)\n",
    "\n",
    "#now I have a column for each image; and the rows are the image vectors/activations from the output layer for that image\n",
    "RDM = 1 - vector_df.corr() #calc dissimilarity score\n",
    "RDM_df = pd.DataFrame(RDM) #transform to dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db3730",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "fontsize = 42\n",
    "Labels = [\"Animal\", \"Zoomorphic\", \"Object\"] #labels of the num_categories\n",
    "\n",
    "plt.setp(ax.get_yticklabels(), fontsize=fontsize)\n",
    "#plt.rcParams[\"font.family\"] = \"Calibri\"\n",
    "plt.rc('font', size=fontsize)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=fontsize)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=fontsize)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=fontsize)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=fontsize)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=fontsize)    # legend fontsize\n",
    "plt.rc('figure', titlesize=fontsize)  # fontsize of the figure titl\n",
    "\n",
    "im = ax.imshow(RDM, cmap='viridis', extent = [0,3,0,3])\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(0.5,3,1))\n",
    "ax.set_yticks(np.arange(0.5,3,1))\n",
    "\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(Labels)\n",
    "ax.set_yticklabels(np.flip(Labels))\n",
    "\n",
    "#title above plot:\n",
    "plt.title('Dissimilarity matrix for FT Animal Bias model', fontsize = fontsize)   \n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_yticklabels(), rotation=90, \n",
    "         fontsize=fontsize, rotation_mode=\"anchor\", ha=\"center\")\n",
    "plt.setp(ax.get_xticklabels(), \n",
    "         fontsize=fontsize, rotation_mode=\"anchor\", ha=\"center\")\n",
    "ax.tick_params(length=0, width=0)\n",
    "\n",
    "#add categorie boundaries\n",
    "colors2 = ['#57a49c', '#97c29f', '#dcddad']\n",
    "ax.hlines(y=-0.1, xmin=0, xmax=1, linewidth=40, color=colors2[0])\n",
    "ax.hlines(y=-0.1, xmin=1, xmax=2, linewidth=40, color=colors2[1])\n",
    "ax.hlines(y=-0.1, xmin=2, xmax=3, linewidth=40, color=colors2[2])\n",
    "\n",
    "ax.vlines(x=-0.1, ymin=0, ymax=1, linewidth=40, color=colors2[2])\n",
    "ax.vlines(x=-0.1, ymin=1, ymax=2, linewidth=40, color=colors2[1])\n",
    "ax.vlines(x=-0.1, ymin=2, ymax=3, linewidth=40, color=colors2[0])\n",
    "\n",
    "\n",
    "cb = fig.colorbar(im, fraction=0.046, pad=0.04)\n",
    "cb.ax.tick_params(labelsize=fontsize)\n",
    "cb.ax.set_ylabel('Distance', rotation=90)\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "    plt.tight_layout()   \n",
    "    #plt.savefig(os.path.join(pathway, title + '.jpg'), bbox_inches='tight', dpi = 600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ed844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
